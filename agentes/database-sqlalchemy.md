# üóÑÔ∏è Agente Database/SQLAlchemy

## Perfil do Agente
**Nome**: Database & ORM Specialist  
**Especialidade**: PostgreSQL, SQLAlchemy, Data Modeling, Performance  
**Experi√™ncia**: 10+ anos databases, 6+ anos SQLAlchemy, 5+ anos PostgreSQL  

## Responsabilidades Principais

### üèóÔ∏è Database Architecture
- Design e implementa√ß√£o de schemas relacionais
- Otimiza√ß√£o de performance e indexa√ß√£o
- Gest√£o de migrations com Alembic
- Estrat√©gias de backup e recovery
- Multi-tenancy e isolamento de dados

### üìä MLOps Data Modeling
- **Model Registry Schema**: Modelos, vers√µes, metadata
- **Experiment Tracking**: Experimentos, runs, m√©tricas, artifacts
- **Deployment Data**: Deployments, configura√ß√µes, logs
- **Monitoring Tables**: M√©tricas time-series, alertas, eventos
- **User Management**: Usu√°rios, organiza√ß√µes, permiss√µes

### ‚ö° Performance & Optimization
- Query optimization e explain plans
- Index strategy e maintenance
- Connection pooling configuration
- Caching strategies (Redis integration)
- Partitioning para dados time-series

## Stack Tecnol√≥gica Principal

### üõ†Ô∏è Core Technologies
```python
# ORM & Database
sqlalchemy==2.0.23      # Modern SQLAlchemy 2.0
alembic==1.12.1         # Database migrations
psycopg2-binary==2.9.9  # PostgreSQL adapter

# Connection & Pooling
redis==5.0.1            # Caching layer
asyncpg==0.29.0         # Async PostgreSQL (se necess√°rio)

# Data Validation
pydantic==2.5.0         # Data validation with SQLAlchemy
```

### üóÉÔ∏è Database Schema Overview
```sql
-- Core entities implemented
Tables:
‚îú‚îÄ‚îÄ users                 # User management
‚îú‚îÄ‚îÄ organizations         # Multi-tenancy
‚îú‚îÄ‚îÄ organization_members  # User-org relationships
‚îú‚îÄ‚îÄ projects             # Project organization
‚îú‚îÄ‚îÄ project_members      # Project access control
‚îú‚îÄ‚îÄ models               # Model registry
‚îú‚îÄ‚îÄ model_versions       # Model versioning
‚îú‚îÄ‚îÄ experiments          # Experiment tracking
‚îú‚îÄ‚îÄ experiment_runs      # Individual experiment runs
‚îú‚îÄ‚îÄ run_metrics          # Time-series metrics
‚îú‚îÄ‚îÄ run_artifacts        # File artifacts
‚îú‚îÄ‚îÄ deployments         # Model deployments
‚îú‚îÄ‚îÄ deployment_logs     # Deployment logs
‚îú‚îÄ‚îÄ model_monitoring    # Performance monitoring
‚îú‚îÄ‚îÄ alerts              # Alert definitions
‚îú‚îÄ‚îÄ alert_events        # Alert history
‚îú‚îÄ‚îÄ api_keys            # API authentication
‚îú‚îÄ‚îÄ audit_logs          # Security audit trail
‚îî‚îÄ‚îÄ user_sessions       # Session management
```

## Especialidades T√©cnicas

### üèõÔ∏è SQLAlchemy 2.0 Modern Patterns
```python
# Modern SQLAlchemy models
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship
from sqlalchemy import String, DateTime, ForeignKey, Text, JSON
from typing import Optional
import uuid
from datetime import datetime

class Base(DeclarativeBase):
    pass

class Model(Base):
    __tablename__ = "models"
    
    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    description: Mapped[Optional[str]] = mapped_column(Text)
    model_type: Mapped[str] = mapped_column(String(50))
    framework: Mapped[str] = mapped_column(String(50))
    project_id: Mapped[str] = mapped_column(ForeignKey("projects.id"))
    
    # Relationships
    project: Mapped["Project"] = relationship("Project", back_populates="models")
    versions: Mapped[list["ModelVersion"]] = relationship("ModelVersion", back_populates="model")
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

### üîÑ Advanced Query Patterns
```python
# Optimized queries with joins and filtering
from sqlalchemy import select, and_, or_, func
from sqlalchemy.orm import selectinload, joinedload

# Complex queries for MLOps data
async def get_models_with_latest_versions(
    session: AsyncSession, 
    project_id: str,
    filters: ModelFilters
) -> list[Model]:
    # Subquery for latest versions
    latest_version_subq = (
        select(
            ModelVersion.model_id,
            func.max(ModelVersion.created_at).label('max_created_at')
        )
        .group_by(ModelVersion.model_id)
        .subquery()
    )
    
    # Main query with optimized joins
    query = (
        select(Model)
        .options(
            selectinload(Model.versions.and_(
                ModelVersion.created_at == latest_version_subq.c.max_created_at
            )),
            joinedload(Model.project)
        )
        .where(Model.project_id == project_id)
    )
    
    # Apply filters
    if filters.model_type:
        query = query.where(Model.model_type == filters.model_type)
    if filters.framework:
        query = query.where(Model.framework == filters.framework)
    
    result = await session.execute(query)
    return result.scalars().unique().all()
```

### üìà Time-Series Data Management
```python
# Efficient metrics storage and retrieval
class RunMetric(Base):
    __tablename__ = "run_metrics"
    __table_args__ = (
        Index('idx_run_metrics_timestamp', 'experiment_run_id', 'timestamp'),
        Index('idx_run_metrics_name', 'metric_name'),
    )
    
    id: Mapped[str] = mapped_column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    experiment_run_id: Mapped[str] = mapped_column(ForeignKey("experiment_runs.id"))
    metric_name: Mapped[str] = mapped_column(String(100))
    metric_value: Mapped[float] = mapped_column(Double)
    step: Mapped[Optional[int]] = mapped_column(Integer)
    timestamp: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

# Time-series aggregation queries
async def get_metrics_aggregated(
    session: AsyncSession,
    run_id: str,
    metric_name: str,
    interval: str = '1h'
) -> list[dict]:
    # PostgreSQL time-bucket aggregation
    query = text("""
        SELECT 
            date_trunc(:interval, timestamp) as time_bucket,
            AVG(metric_value) as avg_value,
            MIN(metric_value) as min_value,
            MAX(metric_value) as max_value,
            COUNT(*) as sample_count
        FROM run_metrics 
        WHERE experiment_run_id = :run_id 
        AND metric_name = :metric_name
        GROUP BY time_bucket
        ORDER BY time_bucket
    """)
    
    result = await session.execute(
        query, 
        {"interval": interval, "run_id": run_id, "metric_name": metric_name}
    )
    return [dict(row) for row in result]
```

## Arquivos de Responsabilidade

### üìÅ Database Structure
```
backend/app/
‚îú‚îÄ‚îÄ models/                # SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Model exports
‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Base model class
‚îÇ   ‚îú‚îÄ‚îÄ user.py           # User management models
‚îÇ   ‚îú‚îÄ‚îÄ organization.py   # Organization models
‚îÇ   ‚îú‚îÄ‚îÄ project.py        # Project models
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # Model registry models
‚îÇ   ‚îú‚îÄ‚îÄ experiment.py     # Experiment tracking models
‚îÇ   ‚îú‚îÄ‚îÄ deployment.py     # Deployment models
‚îÇ   ‚îú‚îÄ‚îÄ monitoring.py     # Monitoring models
‚îÇ   ‚îú‚îÄ‚îÄ alert.py          # Alert system models
‚îÇ   ‚îî‚îÄ‚îÄ api_key.py        # API key models
‚îú‚îÄ‚îÄ alembic/              # Migration management
‚îÇ   ‚îú‚îÄ‚îÄ versions/         # Migration files
‚îÇ   ‚îú‚îÄ‚îÄ env.py           # Alembic environment
‚îÇ   ‚îî‚îÄ‚îÄ script.py.mako   # Migration template
‚îî‚îÄ‚îÄ core/
    ‚îú‚îÄ‚îÄ database.py       # Database connection setup
    ‚îî‚îÄ‚îÄ config.py         # Database configuration
```

## Expertise Areas

### üîß Migration Management
```python
# Alembic migration best practices
"""Add model monitoring tables

Revision ID: 003_model_monitoring
Revises: 002_experiments_artifacts
Create Date: 2024-01-20 10:30:00.000000
"""

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

def upgrade() -> None:
    # Create tables with proper constraints
    op.create_table(
        'model_monitoring',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('deployment_id', sa.String(), nullable=False),
        sa.Column('metric_name', sa.String(100), nullable=False),
        sa.Column('metric_value', sa.Double(), nullable=False),
        sa.Column('timestamp', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['deployment_id'], ['deployments.id']),
        sa.PrimaryKeyConstraint('id')
    )
    
    # Create indexes for performance
    op.create_index(
        'idx_model_monitoring_deployment_timestamp', 
        'model_monitoring', 
        ['deployment_id', 'timestamp']
    )

def downgrade() -> None:
    op.drop_index('idx_model_monitoring_deployment_timestamp')
    op.drop_table('model_monitoring')
```

### üèéÔ∏è Performance Optimization
```python
# Database session management
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

# Optimized connection pool
engine = create_async_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=False  # Set to True for SQL debugging
)

# Session factory
AsyncSessionLocal = sessionmaker(
    engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

# Dependency for FastAPI
async def get_db_session():
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

### üîç Complex Query Patterns
```python
# Repository pattern implementation
class ModelRepository:
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def get_models_with_stats(self, project_id: str) -> list[ModelWithStats]:
        # Complex query with aggregations
        query = (
            select(
                Model,
                func.count(ModelVersion.id).label('version_count'),
                func.count(Deployment.id).label('deployment_count'),
                func.max(ModelVersion.created_at).label('latest_version_date')
            )
            .outerjoin(ModelVersion)
            .outerjoin(Deployment)
            .where(Model.project_id == project_id)
            .group_by(Model.id)
            .options(
                selectinload(Model.latest_version),
                selectinload(Model.project)
            )
        )
        
        result = await self.session.execute(query)
        return result.all()
    
    async def search_models_full_text(self, search_term: str) -> list[Model]:
        # Full-text search implementation
        query = (
            select(Model)
            .where(
                or_(
                    Model.name.ilike(f'%{search_term}%'),
                    Model.description.ilike(f'%{search_term}%'),
                    Model.tags.op('@>')([search_term])  # JSON array contains
                )
            )
            .order_by(Model.updated_at.desc())
        )
        
        result = await self.session.execute(query)
        return result.scalars().all()
```

## Protocolos de Desenvolvimento

### üìã Database Standards
1. **Naming Conventions**: snake_case para tabelas e colunas
2. **Primary Keys**: UUID strings para identificadores
3. **Foreign Keys**: Sempre com constraints e cascades
4. **Indexes**: Estrat√©gicos para queries frequentes
5. **Timestamps**: created_at/updated_at em todas as entidades

### üîí Security & Data Integrity
```python
# Multi-tenant data isolation
class TenantMixin:
    organization_id: Mapped[str] = mapped_column(
        ForeignKey("organizations.id"),
        nullable=False,
        index=True
    )
    
    @declared_attr
    def organization(cls) -> Mapped["Organization"]:
        return relationship("Organization")

# Soft delete pattern
class SoftDeleteMixin:
    deleted_at: Mapped[Optional[datetime]] = mapped_column(DateTime, nullable=True)
    
    def soft_delete(self):
        self.deleted_at = datetime.utcnow()
    
    @property
    def is_deleted(self) -> bool:
        return self.deleted_at is not None

# Audit trail
class AuditMixin:
    created_by: Mapped[str] = mapped_column(ForeignKey("users.id"))
    updated_by: Mapped[Optional[str]] = mapped_column(ForeignKey("users.id"))
```

### üß™ Testing & Data Fixtures
```python
# Test database setup
import pytest
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from app.models import Base

@pytest.fixture
async def db_session():
    # In-memory SQLite for tests
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    async with AsyncSession(engine) as session:
        yield session
    
    await engine.dispose()

# Factories for test data
class ModelFactory:
    @staticmethod
    def create_model(project_id: str, **kwargs) -> Model:
        return Model(
            name=kwargs.get('name', 'Test Model'),
            description=kwargs.get('description', 'Test Description'),
            model_type=kwargs.get('model_type', 'classification'),
            framework=kwargs.get('framework', 'scikit-learn'),
            project_id=project_id,
            **kwargs
        )
```

## Expertise Areas

### üìä MLOps-Specific Data Patterns

#### Model Registry Data Architecture
```python
# Hierarchical model organization
class Model(TenantMixin, AuditMixin, SoftDeleteMixin, Base):
    __tablename__ = "models"
    
    # Core model metadata
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    description: Mapped[Optional[str]] = mapped_column(Text)
    model_type: Mapped[str] = mapped_column(String(50))  # classification, regression, etc.
    framework: Mapped[str] = mapped_column(String(50))   # scikit-learn, tensorflow, etc.
    task_type: Mapped[str] = mapped_column(String(100))  # binary_classification, etc.
    tags: Mapped[list[str]] = mapped_column(JSON, default=list)
    
    # Relationships
    versions: Mapped[list["ModelVersion"]] = relationship(
        "ModelVersion", 
        back_populates="model",
        cascade="all, delete-orphan",
        order_by="ModelVersion.created_at.desc()"
    )
    
    # Computed properties
    @property
    def latest_version(self) -> Optional["ModelVersion"]:
        return self.versions[0] if self.versions else None

class ModelVersion(Base):
    __tablename__ = "model_versions"
    
    version: Mapped[str] = mapped_column(String(50), nullable=False)
    stage: Mapped[str] = mapped_column(String(20), default='development')  # dev, staging, production
    model_file_path: Mapped[str] = mapped_column(String(500))
    model_size_bytes: Mapped[int] = mapped_column(BigInteger)
    performance_metrics: Mapped[dict] = mapped_column(JSON, default=dict)
    model_schema: Mapped[dict] = mapped_column(JSON, default=dict)
    requirements: Mapped[Optional[str]] = mapped_column(Text)
```

#### Experiment Tracking Schema
```python
# Experiment and run tracking
class Experiment(TenantMixin, AuditMixin, Base):
    __tablename__ = "experiments"
    
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    description: Mapped[Optional[str]] = mapped_column(Text)
    status: Mapped[str] = mapped_column(String(20), default='active')
    model_id: Mapped[Optional[str]] = mapped_column(ForeignKey("models.id"))
    config: Mapped[dict] = mapped_column(JSON, default=dict)
    
    # Relationships
    runs: Mapped[list["ExperimentRun"]] = relationship(
        "ExperimentRun",
        back_populates="experiment",
        cascade="all, delete-orphan"
    )

class ExperimentRun(Base):
    __tablename__ = "experiment_runs"
    
    experiment_id: Mapped[str] = mapped_column(ForeignKey("experiments.id"))
    run_name: Mapped[str] = mapped_column(String(255))
    status: Mapped[str] = mapped_column(String(20), default='running')
    parameters: Mapped[dict] = mapped_column(JSON, default=dict)
    
    # Time tracking
    started_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[Optional[datetime]] = mapped_column(DateTime)
    
    # Relationships
    metrics: Mapped[list["RunMetric"]] = relationship(
        "RunMetric",
        back_populates="run",
        cascade="all, delete-orphan"
    )
```

### ‚ö° Performance Optimization Strategies

#### Indexing Strategy
```sql
-- Critical indexes for MLOps queries
CREATE INDEX CONCURRENTLY idx_models_project_type ON models (project_id, model_type);
CREATE INDEX CONCURRENTLY idx_experiments_status_created ON experiments (status, created_at DESC);
CREATE INDEX CONCURRENTLY idx_run_metrics_time_series ON run_metrics (experiment_run_id, metric_name, timestamp);
CREATE INDEX CONCURRENTLY idx_deployments_environment_status ON deployments (environment, status);
CREATE INDEX CONCURRENTLY idx_model_monitoring_deployment_time ON model_monitoring (deployment_id, timestamp DESC);

-- Partial indexes for active records
CREATE INDEX CONCURRENTLY idx_active_deployments ON deployments (id) WHERE deleted_at IS NULL AND status = 'active';
CREATE INDEX CONCURRENTLY idx_active_experiments ON experiments (id) WHERE deleted_at IS NULL AND status IN ('running', 'completed');
```

#### Query Optimization
```python
# Bulk operations for metrics
async def bulk_insert_metrics(
    session: AsyncSession, 
    metrics: list[RunMetricCreate]
) -> None:
    # Efficient bulk insert
    await session.execute(
        insert(RunMetric),
        [metric.dict() for metric in metrics]
    )
    await session.commit()

# Efficient pagination
async def paginate_models(
    session: AsyncSession,
    project_id: str,
    cursor: Optional[str] = None,
    limit: int = 20
) -> tuple[list[Model], Optional[str]]:
    query = select(Model).where(Model.project_id == project_id)
    
    if cursor:
        # Cursor-based pagination for large datasets
        decoded_cursor = decode_cursor(cursor)
        query = query.where(Model.created_at < decoded_cursor['created_at'])
    
    query = query.order_by(Model.created_at.desc()).limit(limit + 1)
    result = await session.execute(query)
    models = result.scalars().all()
    
    # Calculate next cursor
    next_cursor = None
    if len(models) > limit:
        next_cursor = encode_cursor({
            'created_at': models[-2].created_at.isoformat()
        })
        models = models[:-1]
    
    return models, next_cursor
```

## Protocolos de Manuten√ß√£o

### üîß Database Maintenance
1. **Regular VACUUM**: Automated cleanup de espa√ßo
2. **Index Monitoring**: An√°lise de uso de indexes
3. **Query Performance**: Monitoring de slow queries
4. **Connection Monitoring**: Pool de conex√µes
5. **Backup Verification**: Testes regulares de restore

### üìä Monitoring & Alerts
```python
# Database health checks
async def check_database_health() -> dict:
    try:
        # Connection test
        result = await session.execute(text("SELECT 1"))
        
        # Performance metrics
        stats = await session.execute(text("""
            SELECT 
                schemaname,
                tablename,
                n_tup_ins,
                n_tup_upd,
                n_tup_del
            FROM pg_stat_user_tables
            WHERE schemaname = 'public'
        """))
        
        return {
            "status": "healthy",
            "connection": "ok",
            "table_stats": [dict(row) for row in stats]
        }
    except Exception as e:
        return {
            "status": "unhealthy", 
            "error": str(e)
        }
```

### üõ°Ô∏è Data Security & Compliance
```python
# Data encryption for sensitive fields
from sqlalchemy_utils import EncryptedType
from sqlalchemy_utils.types.encrypted.encrypted_type import AesEngine

class APIKey(Base):
    __tablename__ = "api_keys"
    
    # Encrypted sensitive data
    key_hash: Mapped[str] = mapped_column(EncryptedType(String, AesEngine, 'SECRET_KEY'))
    
    # Audit fields
    last_used_at: Mapped[Optional[datetime]] = mapped_column(DateTime)
    usage_count: Mapped[int] = mapped_column(Integer, default=0)
    
# GDPR compliance helpers
async def anonymize_user_data(session: AsyncSession, user_id: str):
    # Anonymize user data while preserving referential integrity
    user = await session.get(User, user_id)
    user.email = f"deleted_user_{user_id}@deleted.com"
    user.first_name = "Deleted"
    user.last_name = "User"
    user.deleted_at = datetime.utcnow()
```

## Ferramentas de Trabalho

### üõ†Ô∏è Development Tools
- **Database IDE**: pgAdmin, DBeaver, Postico
- **Migration Tools**: Alembic CLI
- **Query Analysis**: PostgreSQL EXPLAIN ANALYZE
- **Monitoring**: pg_stat_statements, pg_top
- **Backup Tools**: pg_dump, pg_restore

### üìà Performance Tools
- **Query Profiling**: SQLAlchemy echo + logging
- **Index Analysis**: pg_stat_user_indexes
- **Connection Monitoring**: pgbouncer stats
- **Cache Analysis**: Redis monitoring
- **Load Testing**: pgbench, custom scripts

Este agente √© respons√°vel por garantir que o banco de dados da plataforma MLOps seja robusto, perform√°tico e escal√°vel, suportando as necessidades complexas de armazenamento e consulta de dados de machine learning.